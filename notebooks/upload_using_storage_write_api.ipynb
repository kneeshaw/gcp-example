{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f9c5e2",
   "metadata": {},
   "source": [
    "# Storage Write API Streaming Notebook\n",
    "This notebook rebuilds the proven `bqwapi_upload.py` workflow from scratch so the full dataset can be streamed through the BigQuery Storage Write API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea11d0",
   "metadata": {},
   "source": [
    "## 1. Reset Environment and Imports\n",
    "Reset any cached state, configure logging, and import the minimal set of libraries needed by the working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6261676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import grpc_tools.protoc\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage_v1\n",
    "from google.cloud.bigquery_storage_v1 import types, writer\n",
    "from google.protobuf import descriptor_pb2\n",
    "from google.api_core import exceptions\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"storage_write_notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c1913",
   "metadata": {},
   "source": [
    "## 2. Implement Working Example Function\n",
    "Recreate the streaming helpers from the working script, keeping TIMESTAMP columns as strings exactly like the baseline implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03da0df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'append_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m future = \u001b[43mappend_stream\u001b[49m.send(request)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m     future.result()\n",
      "\u001b[31mNameError\u001b[39m: name 'append_stream' is not defined"
     ]
    }
   ],
   "source": [
    "        future = append_stream.send(request)\n",
    "        try:\n",
    "            future.result()\n",
    "        except exceptions.InvalidArgument as exc:\n",
    "            logger.error(\"AppendRows failed with InvalidArgument: %s\", exc)\n",
    "            if hasattr(exc, \"errors\"):\n",
    "                logger.error(\"Error details: %s\", exc.errors)\n",
    "            logger.error(\"Exception attributes: %s\", dir(exc))\n",
    "            if hasattr(exc, \"details\"):\n",
    "                logger.error(\"details(): %s\", exc.details())\n",
    "            if getattr(exc, \"response\", None) is not None:\n",
    "                logger.error(\"Response object: %s\", exc.response)\n",
    "            if hasattr(exc, \"_details\"):\n",
    "                for detail in exc._details:\n",
    "                    logger.error(\"Detail: %s\", detail)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dbb56a",
   "metadata": {},
   "source": [
    "## 3. Execute Verification Tests\n",
    "Run lightweight assertions that confirm the helpers mirror the working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "883b04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:storage_write_notebook:Proto definition written to /tmp/schema.proto\n",
      "INFO:storage_write_notebook:Proto compiled into /tmp\n",
      "INFO:storage_write_notebook:Proto compiled into /tmp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification checks passed. Proto generated and compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "class DummyField:\n",
    "    def __init__(self, name: str, field_type: str, mode: str = \"NULLABLE\"):\n",
    "        self.name = name\n",
    "        self.field_type = field_type\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "test_schema = [\n",
    "    DummyField(\"vehicle_id\", \"STRING\", \"REQUIRED\"),\n",
    "    DummyField(\"timestamp\", \"TIMESTAMP\", \"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Ensure TIMESTAMP is mapped to string exactly like the working script\n",
    "assert DATA_TYPE_MAPPING[\"TIMESTAMP\"] == \"string\"\n",
    "\n",
    "# Proto file generation should succeed without raising\n",
    "PROTO_FILE.write_text(\"\")\n",
    "generate_proto_file(test_schema, PROTO_FILE)\n",
    "compile_proto(PROTO_FILE)\n",
    "\n",
    "print(\"Verification checks passed. Proto generated and compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99878ff8",
   "metadata": {},
   "source": [
    "## 4. Inspect Example Outputs\n",
    "Fetch live data, build the schema, and stream the entire dataset using the working helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb22169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to Python path so we can import from src modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from ingest.main import run\n",
    "\n",
    "# Ensure the dynamically generated proto module in /tmp is importable\n",
    "if \"/tmp\" not in sys.path:\n",
    "    sys.path.append(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52ca376",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = \"regal-dynamo-470908-v9\"\n",
    "os.environ[\"BQ_DATASET\"] = \"auckland_data_dev\"\n",
    "os.environ[\"BUCKET\"] = \"auckland-data-dev\"\n",
    "\n",
    "# Encode headers as base64 (matching Terraform pattern)\n",
    "import base64\n",
    "_headers_dict = {\n",
    "  'Ocp-Apim-Subscription-Key': '1159c79486524360b17501ad888ee7d6'\n",
    "}\n",
    "os.environ[\"HEADERS\"] = base64.b64encode(json.dumps(_headers_dict).encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "os.environ[\"URL\"] = 'https://api.at.govt.nz/realtime/legacy/vehiclelocations'\n",
    "os.environ[\"DATASET\"] = 'vehicle-positions'\n",
    "os.environ[\"SPEC\"] = 'rt'\n",
    "os.environ[\"RESPONSE_TYPE\"] = 'json'\n",
    "\n",
    "project_id = os.getenv(\"PROJECT_ID\", \"regal-dynamo-470908-v9\")\n",
    "dataset_id = os.getenv(\"BQ_DATASET\", \"auckland_data_dev\")\n",
    "table_id = os.getenv(\"BQ_TABLE\", \"rt_vehicle_positions\")\n",
    "table_fqn = f\"{project_id}.{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a81236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transit_data_pipeline:Starting data ingestion for vehicle-positions...\n",
      "INFO:transit_data_pipeline:Fetched and processed data for vehicle-positions, size: 608019 bytes.\n",
      "INFO:transit_data_pipeline:Fetched and processed data for vehicle-positions, size: 608019 bytes.\n",
      "INFO:transit_data_pipeline:Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=19/hour=20/vehicle-positions-20251019T204009Z.json.gz\n",
      "INFO:transit_data_pipeline:Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=19/hour=20/vehicle-positions-20251019T204009Z.json.gz\n",
      "INFO:transit_data_pipeline:Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO:transit_data_pipeline:Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO:transit_data_pipeline:Transformed data into DataFrame with 1764 records.\n",
      "INFO:transit_data_pipeline:Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO:transit_data_pipeline:Stopped prior to BigQuery upload\n",
      "INFO:storage_write_notebook:Fetched 1764 rows from ingest pipeline\n",
      "INFO:transit_data_pipeline:Transformed data into DataFrame with 1764 records.\n",
      "INFO:transit_data_pipeline:Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO:transit_data_pipeline:Stopped prior to BigQuery upload\n",
      "INFO:storage_write_notebook:Fetched 1764 rows from ingest pipeline\n"
     ]
    }
   ],
   "source": [
    "df = run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64a8650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760907665.470437 5148852 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "INFO:storage_write_notebook:Proto definition written to /tmp/schema_12ab618f044743b3825dda08788765ef.proto\n",
      "INFO:storage_write_notebook:Proto compiled into /tmp\n",
      "INFO:storage_write_notebook:Proto definition written to /tmp/schema_12ab618f044743b3825dda08788765ef.proto\n",
      "INFO:storage_write_notebook:Proto compiled into /tmp\n",
      "I0000 00:00:1760907678.953754 5148852 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1760907678.953754 5148852 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0000 00:00:1760907679.728871 5148852 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760907679.728871 5148852 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "INFO:storage_write_notebook:Created committed write stream projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE\n",
      "INFO:storage_write_notebook:Created committed write stream projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE\n",
      "ERROR:storage_write_notebook:AppendRows failed with InvalidArgument: 400 Errors found while processing rows. Please refer to the row_errors field for details. The list may not be complete because of the size limitations. Entity: projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE\n",
      "ERROR:storage_write_notebook:Error details: []\n",
      "ERROR:storage_write_notebook:Exception attributes: ['__annotations__', '__cause__', '__class__', '__context__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__suppress_context__', '__traceback__', '__weakref__', '_details', '_error_info', '_errors', '_response', 'add_note', 'args', 'code', 'details', 'domain', 'errors', 'grpc_status_code', 'message', 'metadata', 'reason', 'response', 'with_traceback']\n",
      "ERROR:storage_write_notebook:AppendRows failed with InvalidArgument: 400 Errors found while processing rows. Please refer to the row_errors field for details. The list may not be complete because of the size limitations. Entity: projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE\n",
      "ERROR:storage_write_notebook:Error details: []\n",
      "ERROR:storage_write_notebook:Exception attributes: ['__annotations__', '__cause__', '__class__', '__context__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__suppress_context__', '__traceback__', '__weakref__', '_details', '_error_info', '_errors', '_response', 'add_note', 'args', 'code', 'details', 'domain', 'errors', 'grpc_status_code', 'message', 'metadata', 'reason', 'response', 'with_traceback']\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 Errors found while processing rows. Please refer to the row_errors field for details. The list may not be complete because of the size limitations. Entity: projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m compile_proto(PROTO_FILE)\n\u001b[32m     14\u001b[39m start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m rows_written = \u001b[43mstream_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m duration = time.perf_counter() - start\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStreamed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrows_written\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_fqn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mstream_dataframe\u001b[39m\u001b[34m(dataframe, schema_fields, project_id, dataset_id, table_id, batch_size)\u001b[39m\n\u001b[32m    171\u001b[39m future = append_stream.send(request)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    175\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mAppendRows failed with InvalidArgument: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/google/api_core/future/polling.py:261\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry, polling)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, retry=retry, polling=polling)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 Errors found while processing rows. Please refer to the row_errors field for details. The list may not be complete because of the size limitations. Entity: projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cic2YmViNDhlMC0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czE"
     ]
    }
   ],
   "source": [
    "bq_client = bigquery.Client(project=project_id)\n",
    "table = bq_client.get_table(table_fqn)\n",
    "schema_fields = table.schema\n",
    "\n",
    "import uuid\n",
    "_schema_token = uuid.uuid4().hex\n",
    "PROTO_PACKAGE = f\"schema_{_schema_token}\"\n",
    "PROTO_FILE = Path(f\"/tmp/schema_{_schema_token}.proto\")\n",
    "SCHEMA_MODULE = f\"{PROTO_FILE.stem}_pb2\"\n",
    "\n",
    "generate_proto_file(schema_fields, PROTO_FILE)\n",
    "compile_proto(PROTO_FILE)\n",
    "\n",
    "start = time.perf_counter()\n",
    "rows_written = stream_dataframe(df, schema_fields, project_id, dataset_id, table_id)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Streamed {rows_written} rows to {table_fqn} in {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21324898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syntax = \"proto2\";', '', 'package schema;', '', 'message Schema {', '  required string record_id = 1;', '  required string entity_id = 2;', '  required string timestamp = 3;', '  optional sint64 timestamp_s = 4;', '  optional string vehicle_id = 5;', '  optional string vehicle_label = 6;', '  optional string vehicle_license_plate = 7;', '  optional double latitude = 8;', '  optional double longitude = 9;', '  optional double bearing = 10;', '  optional double speed = 11;', '  optional double odometer = 12;', '  optional string occupancy_status = 13;', '  optional string route_id = 14;', '  optional string trip_id = 15;']\n"
     ]
    }
   ],
   "source": [
    "print(Path(PROTO_FILE).read_text().splitlines()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6115fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce TIMESTAMP columns to RFC3339 strings to match Storage Write expectations\n",
    "timestamp_columns = [field.name for field in schema_fields if field.field_type.upper() == \"TIMESTAMP\" and field.name in df.columns]\n",
    "for column in timestamp_columns:\n",
    "    series = pd.to_datetime(df[column], utc=True, errors=\"coerce\")\n",
    "    fallback = pd.Timestamp.now(tz=\"UTC\")\n",
    "    series = series.fillna(fallback)\n",
    "    df[column] = series.dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99da990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill defaults for required fields to avoid missing values during serialization\n",
    "DEFAULT_REQUIRED_VALUES = {\n",
    "    \"odometer\": 0.0,\n",
    "    \"occupancy_status\": \"EMPTY\",\n",
    "    \"route_id\": \"\",\n",
    "    \"trip_id\": \"\",\n",
    "    \"direction_id\": 0,\n",
    "    \"start_date\": \"\",\n",
    "    \"start_time\": \"\",\n",
    "    \"schedule_relationship\": \"SCHEDULED\",\n",
    "}\n",
    "\n",
    "for field in schema_fields:\n",
    "    if field.mode.upper() != \"REQUIRED\":\n",
    "        continue\n",
    "    default_value = DEFAULT_REQUIRED_VALUES.get(field.name)\n",
    "    if field.name not in df.columns:\n",
    "        if default_value is None:\n",
    "            raise ValueError(f\"Required field '{field.name}' missing from DataFrame with no default\")\n",
    "        df[field.name] = default_value\n",
    "        continue\n",
    "    if df[field.name].isna().any():\n",
    "        if default_value is None:\n",
    "            raise ValueError(f\"No default specified for required field '{field.name}' with nulls\")\n",
    "        df[field.name] = df[field.name].fillna(default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca3080b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odometer                 1691\n",
      "occupancy_status          933\n",
      "route_id                  897\n",
      "trip_id                   897\n",
      "direction_id              907\n",
      "start_date                897\n",
      "start_time                897\n",
      "schedule_relationship     897\n",
      "dtype: int64\n",
      "odometer                  NaN\n",
      "occupancy_status          NaN\n",
      "route_id                  NaN\n",
      "trip_id                   NaN\n",
      "direction_id             <NA>\n",
      "start_date                NaN\n",
      "start_time                NaN\n",
      "schedule_relationship     NaN\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "required_columns = [\n",
    "    \"odometer\",\n",
    "    \"occupancy_status\",\n",
    "    \"route_id\",\n",
    "    \"trip_id\",\n",
    "    \"direction_id\",\n",
    "    \"start_date\",\n",
    "    \"start_time\",\n",
    "    \"schedule_relationship\",\n",
    "]\n",
    "missing_summary = df[required_columns].isna().sum()\n",
    "print(missing_summary)\n",
    "print(df.loc[0, required_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2dc5ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2025-10-19T20:45:10.588943+0000', 'updated_at': '2025-10-19T20:45:10.599080+0000', 'record_id': '49f1b0e5436c4505', 'entity_id': 'dc68702249c73592', 'timestamp': '2025-10-19T20:37:19.000000+0000', 'timestamp_s': 1760906239, 'vehicle_id': '518999538', 'vehicle_label': '', 'vehicle_license_plate': '', 'latitude': -36.621502, 'longitude': 174.793742, 'bearing': 199.0, 'speed': 0.0, 'odometer': nan, 'occupancy_status': nan, 'route_id': nan, 'trip_id': nan, 'direction_id': None, 'start_date': nan, 'start_time': nan, 'schedule_relationship': nan}\n"
     ]
    }
   ],
   "source": [
    "print(df.head(1).to_dict(orient=\"records\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec549b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('odometer', 'FLOAT', 'NULLABLE'), ('occupancy_status', 'STRING', 'NULLABLE'), ('route_id', 'STRING', 'NULLABLE'), ('trip_id', 'STRING', 'NULLABLE'), ('direction_id', 'INTEGER', 'NULLABLE'), ('start_date', 'STRING', 'NULLABLE'), ('start_time', 'STRING', 'NULLABLE'), ('schedule_relationship', 'STRING', 'NULLABLE')]\n"
     ]
    }
   ],
   "source": [
    "print([(field.name, field.field_type, field.mode) for field in schema_fields if field.name in required_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28d84b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id', 'entity_id', 'timestamp']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[field.name for field in schema_fields if field.mode.upper() == \"REQUIRED\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
