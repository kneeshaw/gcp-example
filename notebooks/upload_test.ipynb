{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3358feb",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to Python path so we can import from src modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"transit_data_pipeline\")\n",
    "logger.propagate = False        # stop forwarding to the root handler\n",
    "logger.setLevel(logging.INFO)   # pick the level you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from ingest.main import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3bd6fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Starting data ingestion for vehicle-positions...\n",
      "INFO - Fetched and processed data for vehicle-positions, size: 674920 bytes.\n",
      "INFO - Fetched and processed data for vehicle-positions, size: 674920 bytes.\n",
      "INFO - Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=20/hour=19/vehicle-positions-20251020T195245Z.json.gz\n",
      "INFO - Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=20/hour=19/vehicle-positions-20251020T195245Z.json.gz\n",
      "INFO - Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO - Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO - Transformed data into DataFrame with 1893 records.\n",
      "INFO - Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO - Stopped prior to BigQuery upload\n",
      "INFO - Transformed data into DataFrame with 1893 records.\n",
      "INFO - Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO - Stopped prior to BigQuery upload\n"
     ]
    }
   ],
   "source": [
    "# Run existing ingest code to populate dataframe\n",
    "\n",
    "os.environ[\"PROJECT_ID\"] = \"regal-dynamo-470908-v9\"\n",
    "os.environ[\"BQ_DATASET\"] = \"auckland_data_dev\"\n",
    "os.environ[\"BUCKET\"] = \"auckland-data-dev\"\n",
    "\n",
    "# Encode headers as base64 (matching Terraform pattern)\n",
    "import base64\n",
    "_headers_dict = {\n",
    "  'Ocp-Apim-Subscription-Key': '1159c79486524360b17501ad888ee7d6'\n",
    "}\n",
    "os.environ[\"HEADERS\"] = base64.b64encode(json.dumps(_headers_dict).encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "os.environ[\"URL\"] = 'https://api.at.govt.nz/realtime/legacy/vehiclelocations'\n",
    "os.environ[\"DATASET\"] = 'vehicle-positions'\n",
    "os.environ[\"SPEC\"] = 'rt'\n",
    "os.environ[\"RESPONSE_TYPE\"] = 'json'\n",
    "\n",
    "project_id = \"regal-dynamo-470908-v9\"\n",
    "dataset = \"auckland_data_dev\"\n",
    "table_name = 'rt_vehicle_positions'\n",
    "table_id = f\"{project_id}.{dataset}.{table_name}\"\n",
    "\n",
    "from ingest.main import run\n",
    "df = run({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b8a42",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383bbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_query.methods.batch_method import upload as batch_upload\n",
    "from big_query.methods.streaming_method import upload as streaming_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d22991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch upload took 6.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "_start = time.perf_counter()\n",
    "\n",
    "batch_upload(df, project_id, dataset, table_name)\n",
    "\n",
    "elapsed = time.perf_counter() - _start\n",
    "print(f\"Batch upload took {elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8074ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:201: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  key_frame = source_df[list(key_columns)].copy().applymap(_normalize_key_value).drop_duplicates()\n",
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:203: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  payload = key_frame.applymap(_serialize_snapshot_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming upload took 7.37 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "_start = time.perf_counter()\n",
    "\n",
    "streaming_upload(\n",
    "    df,\n",
    "    project_id,\n",
    "    dataset,\n",
    "    table_name,\n",
    "    use_snapshot=True,\n",
    "    snapshot_bucket=\"auckland-data-dev\",\n",
    "    snapshot_object=\"snapshot/rt_vehicle_position.csv\",\n",
    ")\n",
    "\n",
    "elapsed = time.perf_counter() - _start\n",
    "print(f\"Streaming upload took {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1db8db",
   "metadata": {},
   "source": [
    "### Snapshot Profiling\n",
    "Below helper runs the snapshot-enabled streaming pipeline step by step on a sample batch and records how long each stage takes. Adjust the sample slice before rerunning if you want to limit duplicate inserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d13951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:201: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  key_frame = source_df[list(key_columns)].copy().applymap(_normalize_key_value).drop_duplicates()\n",
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:203: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  payload = key_frame.applymap(_serialize_snapshot_value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timings': OrderedDict([('prepare_snapshot', 0.00018445804016664624),\n",
       "              ('load_snapshot_keys', 2.604979290976189),\n",
       "              ('drop_seen_rows', 0.00961237499723211),\n",
       "              ('serialize_rows', 0.10402949998388067),\n",
       "              ('insert_rows', 0.6582888749544509),\n",
       "              ('write_snapshot', 2.467181458021514)]),\n",
       " 'stats': {'snapshot_key_count': 1916,\n",
       "  'skipped_duplicates': 111,\n",
       "  'rows_after_dedupe': 1802,\n",
       "  'insert_error_count': 0},\n",
       " 'errors': [],\n",
       " 'table': 'regal-dynamo-470908-v9.auckland_data_dev.rt_vehicle_positions',\n",
       " 'snapshot_uri': 'gs://auckland-data-dev/snapshot/rt_vehicle_position.csv'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "from big_query.methods import streaming_method\n",
    "from google.cloud import bigquery\n",
    "\n",
    "SNAPSHOT_BUCKET = \"auckland-data-dev\"\n",
    "SNAPSHOT_OBJECT = \"snapshot/rt_vehicle_position.csv\"\n",
    "\n",
    "\n",
    "def profile_snapshot_stream(\n",
    "    batch_df: pd.DataFrame,\n",
    "    project: str,\n",
    "    dataset_name: str,\n",
    "    table: str,\n",
    "    *,\n",
    "    snapshot_bucket: str,\n",
    "    snapshot_object: str,\n",
    "    key_columns=None,\n",
    "):\n",
    "    \"\"\"Return per-stage timings for the snapshot-enabled streaming upload.\"\"\"\n",
    "    timings = OrderedDict()\n",
    "    stats = {}\n",
    "\n",
    "    key_columns = list(key_columns or streaming_method.DEFAULT_SNAPSHOT_KEYS)\n",
    "    table_id = f\"{project}.{dataset_name}.{table}\"\n",
    "    snapshot_uri = f\"gs://{snapshot_bucket}/{snapshot_object}\"\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "    bq_table = client.get_table(table_id)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    schema_map = {field.name: field for field in bq_table.schema}\n",
    "    timings[\"prepare_snapshot\"] = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    snapshot_keys = streaming_method._load_snapshot_keys(\n",
    "        snapshot_bucket, snapshot_object, key_columns, schema_map\n",
    "    )\n",
    "    timings[\"load_snapshot_keys\"] = time.perf_counter() - start\n",
    "    stats[\"snapshot_key_count\"] = len(snapshot_keys)\n",
    "\n",
    "    working_df = batch_df.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    deduped_df, skipped = streaming_method._drop_seen_rows(\n",
    "        working_df, snapshot_keys, key_columns\n",
    "    )\n",
    "    timings[\"drop_seen_rows\"] = time.perf_counter() - start\n",
    "    stats[\"skipped_duplicates\"] = skipped\n",
    "    stats[\"rows_after_dedupe\"] = len(deduped_df)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rows_payload = streaming_method._dataframe_to_rows(deduped_df)\n",
    "    timings[\"serialize_rows\"] = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    insert_errors = []\n",
    "    if rows_payload:\n",
    "        insert_errors = client.insert_rows(bq_table, rows_payload)\n",
    "    timings[\"insert_rows\"] = time.perf_counter() - start\n",
    "    stats[\"insert_error_count\"] = len(insert_errors)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    streaming_method._overwrite_snapshot(\n",
    "        snapshot_bucket, snapshot_object, working_df, key_columns\n",
    "    )\n",
    "    timings[\"write_snapshot\"] = time.perf_counter() - start\n",
    "\n",
    "    return {\n",
    "        \"timings\": timings,\n",
    "        \"stats\": stats,\n",
    "        \"errors\": insert_errors,\n",
    "        \"table\": table_id,\n",
    "        \"snapshot_uri\": snapshot_uri,\n",
    "    }\n",
    "\n",
    "\n",
    "# Use all rows by default; adjust sample if you want to limit duplicate inserts\n",
    "profiling_result = profile_snapshot_stream(\n",
    "    df,\n",
    "    project_id,\n",
    "    dataset,\n",
    "    table_name,\n",
    "    snapshot_bucket=SNAPSHOT_BUCKET,\n",
    "    snapshot_object=SNAPSHOT_OBJECT,\n",
    ")\n",
    "profiling_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
