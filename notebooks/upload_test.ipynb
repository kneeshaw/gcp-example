{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3358feb",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to Python path so we can import from src modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"transit_data_pipeline\")\n",
    "logger.propagate = False        # stop forwarding to the root handler\n",
    "logger.setLevel(logging.INFO)   # pick the level you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from ingest.main import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bd6fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Starting data ingestion for vehicle-positions...\n",
      "INFO - Fetched and processed data for vehicle-positions, size: 562152 bytes.\n",
      "INFO - Fetched and processed data for vehicle-positions, size: 562152 bytes.\n",
      "INFO - Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=20/hour=21/vehicle-positions-20251020T211641Z.json.gz\n",
      "INFO - Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=20/hour=21/vehicle-positions-20251020T211641Z.json.gz\n",
      "INFO - Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO - Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO - Transformed data into DataFrame with 1671 records.\n",
      "INFO - Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO - Stopped prior to BigQuery upload\n",
      "INFO - Transformed data into DataFrame with 1671 records.\n",
      "INFO - Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO - Stopped prior to BigQuery upload\n"
     ]
    }
   ],
   "source": [
    "# Run existing ingest code to populate dataframe\n",
    "\n",
    "os.environ[\"PROJECT_ID\"] = \"regal-dynamo-470908-v9\"\n",
    "os.environ[\"BQ_DATASET\"] = \"auckland_data_dev\"\n",
    "os.environ[\"BUCKET\"] = \"auckland-data-dev\"\n",
    "\n",
    "# Encode headers as base64 (matching Terraform pattern)\n",
    "import base64\n",
    "_headers_dict = {\n",
    "  'Ocp-Apim-Subscription-Key': '1159c79486524360b17501ad888ee7d6'\n",
    "}\n",
    "os.environ[\"HEADERS\"] = base64.b64encode(json.dumps(_headers_dict).encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "os.environ[\"URL\"] = 'https://api.at.govt.nz/realtime/legacy/vehiclelocations'\n",
    "os.environ[\"DATASET\"] = 'vehicle-positions'\n",
    "os.environ[\"SPEC\"] = 'rt'\n",
    "os.environ[\"RESPONSE_TYPE\"] = 'json'\n",
    "\n",
    "project_id = \"regal-dynamo-470908-v9\"\n",
    "dataset = \"auckland_data_dev\"\n",
    "table_name = 'rt_vehicle_positions'\n",
    "table_id = f\"{project_id}.{dataset}.{table_name}\"\n",
    "\n",
    "from ingest.main import run\n",
    "df = run({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b8a42",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383bbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_query.methods.batch_method import upload as batch_upload\n",
    "#from big_query.methods.streaming_method import upload as streaming_upload\n",
    "from big_query.methods.method_treaming_redis_snapshot import (\n",
    "    upload as redis_snapshot_upload,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d22991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch upload took 7.56 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "_start = time.perf_counter()\n",
    "\n",
    "batch_upload(df, project_id, dataset, table_name)\n",
    "\n",
    "elapsed = time.perf_counter() - _start\n",
    "print(f\"Batch upload took {elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8074ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'streaming_upload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m _start = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mstreaming_upload\u001b[49m(\n\u001b[32m      6\u001b[39m     df,\n\u001b[32m      7\u001b[39m     project_id,\n\u001b[32m      8\u001b[39m     dataset,\n\u001b[32m      9\u001b[39m     table_name,\n\u001b[32m     10\u001b[39m     use_snapshot=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     snapshot_bucket=\u001b[33m\"\u001b[39m\u001b[33mauckland-data-dev\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     snapshot_object=\u001b[33m\"\u001b[39m\u001b[33msnapshot/rt_vehicle_position.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m elapsed = time.perf_counter() - _start\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStreaming upload took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'streaming_upload' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "_start = time.perf_counter()\n",
    "\n",
    "streaming_upload(\n",
    "    df,\n",
    "    project_id,\n",
    "    dataset,\n",
    "    table_name,\n",
    "    use_snapshot=True,\n",
    "    snapshot_bucket=\"auckland-data-dev\",\n",
    "    snapshot_object=\"snapshot/rt_vehicle_position.csv\",\n",
    ")\n",
    "\n",
    "elapsed = time.perf_counter() - _start\n",
    "print(f\"Streaming upload took {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5256f",
   "metadata": {},
   "source": [
    "### Redis Snapshot Streaming\n",
    "Use the Redis-backed snapshot flow to test deduplication without touching the legacy helper. Populate the connection details from your Memorystore instance or the outputs generated during `terraform apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f6b7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'redis_url': 'redis://:cdcad05e-27a1-49c3-bdfb-5d32418bda08@10.146.65.252:6378',\n",
       " 'redis_snapshot_key': 'regal-dynamo-470908-v9:auckland_data_dev:rt_vehicle_positions:snapshot',\n",
       " 'snapshot_ttl_seconds': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "\n",
    "redis_host = os.getenv(\"REDIS_HOST\")\n",
    "redis_port = int(os.getenv(\"REDIS_PORT\", \"6379\"))\n",
    "redis_auth = os.getenv(\"REDIS_AUTH\")\n",
    "redis_use_tls = os.getenv(\"REDIS_USE_TLS\", \"false\").lower() in {\"true\", \"1\", \"yes\"}\n",
    "\n",
    "# Allow notebook-local files to supply connection info when env vars are absent\n",
    "base_dir = Path.cwd()\n",
    "details_path = base_dir / \"redis_details.json\"\n",
    "auth_path = base_dir / \"redis_auth_string.txt\"\n",
    "\n",
    "if (not redis_host or redis_host == \"\") and details_path.exists():\n",
    "    with details_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        details = json.load(fh)\n",
    "    redis_host = details.get(\"host\", redis_host)\n",
    "    redis_port = int(details.get(\"port\", redis_port))\n",
    "    # Memorystore defaults to TLS disabled unless configured\n",
    "    redis_use_tls = bool(details.get(\"use_tls\", redis_use_tls))\n",
    "\n",
    "if (not redis_auth or redis_auth == \"\") and auth_path.exists():\n",
    "    raw_auth = auth_path.read_text(encoding=\"utf-8\").strip()\n",
    "    if raw_auth:\n",
    "        try:\n",
    "            redis_auth = json.loads(raw_auth)\n",
    "        except json.JSONDecodeError:\n",
    "            redis_auth = raw_auth\n",
    "\n",
    "if not redis_host:\n",
    "    raise RuntimeError(\n",
    "        \"Set REDIS_HOST (or ensure redis_details.json is available) before running the Redis snapshot test.\"\n",
    "    )\n",
    "\n",
    "auth_fragment = f\":{quote(redis_auth)}@\" if redis_auth else \"\"\n",
    "scheme = \"rediss\" if redis_use_tls else \"redis\"\n",
    "redis_url = f\"{scheme}://{auth_fragment}{redis_host}:{redis_port}\"\n",
    "\n",
    "redis_snapshot_key = os.getenv(\n",
    "    \"REDIS_SNAPSHOT_KEY\", f\"{project_id}:{dataset}:{table_name}:snapshot\"\n",
    ")\n",
    "ttl_env = os.getenv(\"REDIS_SNAPSHOT_TTL_SECONDS\")\n",
    "snapshot_ttl_seconds = int(ttl_env) if ttl_env else None\n",
    "\n",
    "redis_config = {\n",
    "    \"redis_url\": redis_url,\n",
    "    \"redis_snapshot_key\": redis_snapshot_key,\n",
    "    \"snapshot_ttl_seconds\": snapshot_ttl_seconds,\n",
    "}\n",
    "redis_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496a134f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 65 connecting to 10.146.65.252:6378. No route to host.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:389\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/retry.py:105\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:390\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_socket_connect:\n\u001b[32m    389\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m.disconnect(error)\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:803\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msocket.getaddrinfo returned an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:787\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msocket_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 65] No route to host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbig_query\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmethods\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmethod_treaming_redis_snapshot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     upload \u001b[38;5;28;01mas\u001b[39;00m redis_snapshot_upload,\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m _start = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m redis_result = \u001b[43mredis_snapshot_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_snapshot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mredis_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredis_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mredis_url\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mredis_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredis_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mredis_snapshot_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msnapshot_ttl_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredis_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msnapshot_ttl_seconds\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m elapsed = time.perf_counter() - _start\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRedis snapshot streaming took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/method_treaming_redis_snapshot.py:66\u001b[39m, in \u001b[36mupload\u001b[39m\u001b[34m(df, project_id, dataset, table_name, use_snapshot, snapshot_key_columns, redis_url, redis_key, redis_client, snapshot_ttl_seconds)\u001b[39m\n\u001b[32m     63\u001b[39m snapshot_resource = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mredis://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mredis_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m schema_map = {field.name: field \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m table.schema}\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m snapshot_keys = \u001b[43m_load_snapshot_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredis_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredis_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m df, skipped_duplicates = _drop_seen_rows(df, snapshot_keys, key_columns)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/method_treaming_redis_snapshot.py:147\u001b[39m, in \u001b[36m_load_snapshot_keys\u001b[39m\u001b[34m(client, redis_key, key_columns, schema_map)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_columns:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m members = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msmembers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredis_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m members:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/commands/core.py:3384\u001b[39m, in \u001b[36mSetCommands.smembers\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   3378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msmembers\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: KeyT) -> Union[Awaitable[Set], Set]:\n\u001b[32m   3379\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3380\u001b[39m \u001b[33;03m    Return all members of the set ``name``\u001b[39;00m\n\u001b[32m   3381\u001b[39m \n\u001b[32m   3382\u001b[39m \u001b[33;03m    For more information see https://redis.io/commands/smembers\u001b[39;00m\n\u001b[32m   3383\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3384\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSMEMBERS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/client.py:621\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/client.py:627\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    625\u001b[39m pool = \u001b[38;5;28mself\u001b[39m.connection_pool\n\u001b[32m    626\u001b[39m command_name = args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.connection \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/utils.py:195\u001b[39m, in \u001b[36mdeprecated_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m provided_args:\n\u001b[32m    191\u001b[39m         warn_deprecated_arg_usage(\n\u001b[32m    192\u001b[39m             arg, func.\u001b[34m__name__\u001b[39m, reason, version, stacklevel=\u001b[32m3\u001b[39m\n\u001b[32m    193\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:1533\u001b[39m, in \u001b[36mConnectionPool.get_connection\u001b[39m\u001b[34m(self, command_name, *keys, **options)\u001b[39m\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_use_connections.add(connection)\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1532\u001b[39m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[32m   1536\u001b[39m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:380\u001b[39m, in \u001b[36mAbstractConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConnects to the Redis server if not already connected\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect_check_health\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_health\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mak-group/gcp-example/notebooks/.venv/lib/python3.11/site-packages/redis/connection.py:397\u001b[39m, in \u001b[36mAbstractConnection.connect_check_health\u001b[39m\u001b[34m(self, check_health, retry_socket_connect)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout connecting to server\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._error_message(e))\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._sock = sock\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mConnectionError\u001b[39m: Error 65 connecting to 10.146.65.252:6378. No route to host."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from big_query.methods.method_treaming_redis_snapshot import (\n",
    "    upload as redis_snapshot_upload,\n",
    ")\n",
    "\n",
    "_start = time.perf_counter()\n",
    "\n",
    "redis_result = redis_snapshot_upload(\n",
    "    df,\n",
    "    project_id,\n",
    "    dataset,\n",
    "    table_name,\n",
    "    use_snapshot=True,\n",
    "    redis_url=redis_config[\"redis_url\"],\n",
    "    redis_key=redis_config[\"redis_snapshot_key\"],\n",
    "    snapshot_ttl_seconds=redis_config[\"snapshot_ttl_seconds\"],\n",
    ")\n",
    "\n",
    "elapsed = time.perf_counter() - _start\n",
    "print(f\"Redis snapshot streaming took {elapsed:.2f} seconds\")\n",
    "redis_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1db8db",
   "metadata": {},
   "source": [
    "### Snapshot Profiling\n",
    "Below helper runs the snapshot-enabled streaming pipeline step by step on a sample batch and records how long each stage takes. Adjust the sample slice before rerunning if you want to limit duplicate inserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d13951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:201: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  key_frame = source_df[list(key_columns)].copy().applymap(_normalize_key_value).drop_duplicates()\n",
      "/Users/martin/Projects/mak-group/gcp-example/notebooks/../src/big_query/methods/streaming_method.py:203: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  payload = key_frame.applymap(_serialize_snapshot_value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timings': OrderedDict([('prepare_snapshot', 0.00018445804016664624),\n",
       "              ('load_snapshot_keys', 2.604979290976189),\n",
       "              ('drop_seen_rows', 0.00961237499723211),\n",
       "              ('serialize_rows', 0.10402949998388067),\n",
       "              ('insert_rows', 0.6582888749544509),\n",
       "              ('write_snapshot', 2.467181458021514)]),\n",
       " 'stats': {'snapshot_key_count': 1916,\n",
       "  'skipped_duplicates': 111,\n",
       "  'rows_after_dedupe': 1802,\n",
       "  'insert_error_count': 0},\n",
       " 'errors': [],\n",
       " 'table': 'regal-dynamo-470908-v9.auckland_data_dev.rt_vehicle_positions',\n",
       " 'snapshot_uri': 'gs://auckland-data-dev/snapshot/rt_vehicle_position.csv'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "from big_query.methods import streaming_method\n",
    "from google.cloud import bigquery\n",
    "\n",
    "SNAPSHOT_BUCKET = \"auckland-data-dev\"\n",
    "SNAPSHOT_OBJECT = \"snapshot/rt_vehicle_position.csv\"\n",
    "\n",
    "\n",
    "def profile_snapshot_stream(\n",
    "    batch_df: pd.DataFrame,\n",
    "    project: str,\n",
    "    dataset_name: str,\n",
    "    table: str,\n",
    "    *,\n",
    "    snapshot_bucket: str,\n",
    "    snapshot_object: str,\n",
    "    key_columns=None,\n",
    "):\n",
    "    \"\"\"Return per-stage timings for the snapshot-enabled streaming upload.\"\"\"\n",
    "    timings = OrderedDict()\n",
    "    stats = {}\n",
    "\n",
    "    key_columns = list(key_columns or streaming_method.DEFAULT_SNAPSHOT_KEYS)\n",
    "    table_id = f\"{project}.{dataset_name}.{table}\"\n",
    "    snapshot_uri = f\"gs://{snapshot_bucket}/{snapshot_object}\"\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "    bq_table = client.get_table(table_id)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    schema_map = {field.name: field for field in bq_table.schema}\n",
    "    timings[\"prepare_snapshot\"] = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    snapshot_keys = streaming_method._load_snapshot_keys(\n",
    "        snapshot_bucket, snapshot_object, key_columns, schema_map\n",
    "    )\n",
    "    timings[\"load_snapshot_keys\"] = time.perf_counter() - start\n",
    "    stats[\"snapshot_key_count\"] = len(snapshot_keys)\n",
    "\n",
    "    working_df = batch_df.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    deduped_df, skipped = streaming_method._drop_seen_rows(\n",
    "        working_df, snapshot_keys, key_columns\n",
    "    )\n",
    "    timings[\"drop_seen_rows\"] = time.perf_counter() - start\n",
    "    stats[\"skipped_duplicates\"] = skipped\n",
    "    stats[\"rows_after_dedupe\"] = len(deduped_df)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rows_payload = streaming_method._dataframe_to_rows(deduped_df)\n",
    "    timings[\"serialize_rows\"] = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    insert_errors = []\n",
    "    if rows_payload:\n",
    "        insert_errors = client.insert_rows(bq_table, rows_payload)\n",
    "    timings[\"insert_rows\"] = time.perf_counter() - start\n",
    "    stats[\"insert_error_count\"] = len(insert_errors)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    streaming_method._overwrite_snapshot(\n",
    "        snapshot_bucket, snapshot_object, working_df, key_columns\n",
    "    )\n",
    "    timings[\"write_snapshot\"] = time.perf_counter() - start\n",
    "\n",
    "    return {\n",
    "        \"timings\": timings,\n",
    "        \"stats\": stats,\n",
    "        \"errors\": insert_errors,\n",
    "        \"table\": table_id,\n",
    "        \"snapshot_uri\": snapshot_uri,\n",
    "    }\n",
    "\n",
    "\n",
    "# Use all rows by default; adjust sample if you want to limit duplicate inserts\n",
    "profiling_result = profile_snapshot_stream(\n",
    "    df,\n",
    "    project_id,\n",
    "    dataset,\n",
    "    table_name,\n",
    "    snapshot_bucket=SNAPSHOT_BUCKET,\n",
    "    snapshot_object=SNAPSHOT_OBJECT,\n",
    ")\n",
    "profiling_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
