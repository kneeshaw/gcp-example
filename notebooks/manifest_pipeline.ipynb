{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5bd508",
   "metadata": {},
   "source": [
    "# Manifest-Orchestrated Ingestion Pipeline\n",
    "This notebook prototypes a staged workflow for capturing raw realtime feeds into GCS, tracking them via a BigQuery manifest, and batching curated upserts without relying on Redis or the Storage Write API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bf2fd",
   "metadata": {},
   "source": [
    "## Working Plan\n",
    "We will iterate through four stages, validating each before productionising:\n",
    "1. **Manifest schema + helpers** – design the BigQuery table layout and author idempotent insert/update utilities.\n",
    "2. **Raw fetch simulation** – exercise current ingestion logic to write sample manifest rows while landing files in GCS.\n",
    "3. **Batch assembly strategies** – evaluate parallel downloads versus server-side `gcloud storage objects compose` for bundling raw files.\n",
    "4. **Batch load + upsert** – reuse the existing BigQuery batch uploader to load a staging table and run partition-scoped MERGEs, finishing by updating manifest statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26a47b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: regal-dynamo-470908-v9\n",
      "Manifest table: regal-dynamo-470908-v9.rt_manifest_dev.ingestion_manifest\n",
      "Raw bucket prefix: gs://auckland-data-dev/raw/vehicle_positions/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialise shared clients and common configuration for the staged experiments.\"\"\"\n",
    "from __future__ import annotations\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery, storage\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\", \"regal-dynamo-470908-v9\")\n",
    "BQ_DATASET = os.environ.get(\"BQ_MANIFEST_DATASET\", \"rt_manifest_dev\")\n",
    "MANIFEST_TABLE = f\"{PROJECT_ID}.{BQ_DATASET}.ingestion_manifest\"\n",
    "RAW_BUCKET = os.environ.get(\"RAW_BUCKET\", \"auckland-data-dev\")\n",
    "RAW_PREFIX = os.environ.get(\"RAW_PREFIX\", \"raw/vehicle_positions/\")\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "gcs_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "print(\"Project:\", PROJECT_ID)\n",
    "print(\"Manifest table:\", MANIFEST_TABLE)\n",
    "print(\"Raw bucket prefix:\", f\"gs://{RAW_BUCKET}/{RAW_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceaea94",
   "metadata": {},
   "source": [
    "## Stage 1 · Manifest schema and helpers\n",
    "Goals for this section:\n",
    "- Declare the manifest table schema (columns, partitioning, clustering).\n",
    "- Create utility functions to upsert manifest rows idempotently using standard BigQuery inserts.\n",
    "- Smoke-test by writing and reading back a sample entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c434cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest table is ready.\n"
     ]
    }
   ],
   "source": [
    "MANIFEST_SCHEMA = [\n",
    "    bigquery.SchemaField(\"dataset\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"object_uri\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"capture_ts\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"partition_key\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"status\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"status_updated_ts\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"checksum\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"payload_bytes\", \"INT64\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"attributes\", \"JSON\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "def ensure_manifest_table() -> None:\n",
    "    \"\"\"Create the manifest dataset/table if they do not yet exist.\"\"\"\n",
    "    dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET}\")\n",
    "    dataset_ref.location = \"australia-southeast1\"\n",
    "    try:\n",
    "        bq_client.get_dataset(dataset_ref)\n",
    "    except Exception:\n",
    "        bq_client.create_dataset(dataset_ref, exists_ok=True)\n",
    "\n",
    "    table = bigquery.Table(MANIFEST_TABLE, schema=MANIFEST_SCHEMA)\n",
    "    table.time_partitioning = bigquery.TimePartitioning(field=\"capture_ts\", type_=bigquery.TimePartitioningType.HOUR)\n",
    "    table.clustering_fields = [\"dataset\", \"partition_key\"]\n",
    "    bq_client.create_table(table, exists_ok=True)\n",
    "\n",
    "ensure_manifest_table()\n",
    "print(\"Manifest table is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cff53283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_manifest_rows(rows: Iterable[dict]) -> None:\n",
    "    \"\"\"Insert or update manifest rows using DML MERGE on the primary key (dataset, object_uri).\"\"\"\n",
    "    rows = list(rows)\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    table = bigquery.TableReference.from_string(MANIFEST_TABLE)\n",
    "    temp_table_id = f\"{MANIFEST_TABLE}_staging\"\n",
    "    staging_table = bigquery.Table(temp_table_id, schema=MANIFEST_SCHEMA)\n",
    "    staging_table.time_partitioning = bigquery.TimePartitioning(field=\"capture_ts\", type_=bigquery.TimePartitioningType.HOUR)\n",
    "    staging_table.clustering_fields = [\"dataset\", \"partition_key\"]\n",
    "    bq_client.create_table(staging_table, exists_ok=True)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        schema=MANIFEST_SCHEMA,\n",
    "    )\n",
    "    job = bq_client.load_table_from_json(\n",
    "        rows,\n",
    "        temp_table_id,\n",
    "        job_config=job_config,\n",
    "    )\n",
    "    job.result()\n",
    "\n",
    "    merge_sql = f\"\"\"\\n    MERGE `{MANIFEST_TABLE}` AS target\\n    USING `{temp_table_id}` AS source\\n      ON target.dataset = source.dataset\\n     AND target.object_uri = source.object_uri\\n    WHEN MATCHED THEN\\n      UPDATE SET\\n        status = source.status,\\n        status_updated_ts = source.status_updated_ts,\\n        checksum = source.checksum,\\n        payload_bytes = source.payload_bytes,\\n        attributes = TO_JSON(source.attributes)\\n    WHEN NOT MATCHED THEN\\n      INSERT (`dataset`, `object_uri`, `capture_ts`, `partition_key`, `status`, `status_updated_ts`, `checksum`, `payload_bytes`, `attributes`)\\n      VALUES (source.dataset, source.object_uri, source.capture_ts, CAST(source.partition_key AS STRING), source.status, source.status_updated_ts, source.checksum, source.payload_bytes, TO_JSON(source.attributes))\\n    \"\"\"\n",
    "    bq_client.query(merge_sql).result()\n",
    "    bq_client.delete_table(temp_table_id, not_found_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "050c0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761075598.490796 6836922 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_uri",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status_updated_ts",
         "rawType": "datetime64[us, UTC]",
         "type": "unknown"
        }
       ],
       "ref": "2af65fd5-4bed-4388-ad85-6cb28ad4b67f",
       "rows": [
        [
         "0",
         "vehicle-positions",
         "gs://sample-bucket/raw/vehicle_positions/2025-10-21T22:00:05Z.json",
         "new",
         "2025-10-21 19:39:48.560749+00:00"
        ],
        [
         "1",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1237-04.json",
         "new",
         "2025-10-21 19:12:37.183684+00:00"
        ],
        [
         "2",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-03.json",
         "new",
         "2025-10-21 19:12:36.888786+00:00"
        ],
        [
         "3",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-02.json",
         "new",
         "2025-10-21 19:12:36.498692+00:00"
        ],
        [
         "4",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-01.json",
         "new",
         "2025-10-21 19:12:36.191643+00:00"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>object_uri</th>\n",
       "      <th>status</th>\n",
       "      <th>status_updated_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://sample-bucket/raw/vehicle_positions/2025-...</td>\n",
       "      <td>new</td>\n",
       "      <td>2025-10-21 19:39:48.560749+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>2025-10-21 19:12:37.183684+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>2025-10-21 19:12:36.888786+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>2025-10-21 19:12:36.498692+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>2025-10-21 19:12:36.191643+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset                                         object_uri  \\\n",
       "0  vehicle-positions  gs://sample-bucket/raw/vehicle_positions/2025-...   \n",
       "1  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "2  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "3  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "4  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "\n",
       "  status                status_updated_ts  \n",
       "0    new 2025-10-21 19:39:48.560749+00:00  \n",
       "1    new 2025-10-21 19:12:37.183684+00:00  \n",
       "2    new 2025-10-21 19:12:36.888786+00:00  \n",
       "3    new 2025-10-21 19:12:36.498692+00:00  \n",
       "4    new 2025-10-21 19:12:36.191643+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_row = {\n",
    "    \"dataset\": \"vehicle-positions\",\n",
    "    \"object_uri\": \"gs://sample-bucket/raw/vehicle_positions/2025-10-21T22:00:05Z.json\",\n",
    "    \"capture_ts\": dt.datetime.utcnow().isoformat(),\n",
    "    \"partition_key\": \"2025-10-21T22:00\",\n",
    "    \"status\": \"new\",\n",
    "    \"status_updated_ts\": dt.datetime.utcnow().isoformat(),\n",
    "    \"checksum\": \"deadbeef\",\n",
    "    \"payload_bytes\": 12345,\n",
    "    \"attributes\": {\"notes\": \"Stage1 smoke test\"},\n",
    "}\n",
    "\n",
    "upsert_manifest_rows([sample_row])\n",
    "display(bq_client.query(\n",
    "    f\"SELECT dataset, object_uri, status, status_updated_ts FROM `{MANIFEST_TABLE}` ORDER BY status_updated_ts DESC LIMIT 5\"\n",
    ").to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6632ed",
   "metadata": {},
   "source": [
    "## Next · Stage 2 setup\n",
    "With the manifest scaffolding ready, the next step will simulate raw ingestion:\n",
    "- Reuse existing fetch/transform code to land a handful of vehicle-position snapshots in GCS.\n",
    "- Record their URIs via `upsert_manifest_rows`, validating idempotency and timing metrics.\n",
    "We will add these cells once you’re ready to move into Stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c154c",
   "metadata": {},
   "source": [
    "## Stage 2 · Simulate raw ingestion\n",
    "Objectives:\n",
    "- Pull a handful of recent vehicle-position snapshots (or synthetic payloads) into the raw bucket.\n",
    "- Record each object via `upsert_manifest_rows`, capturing partition keys and sizes.\n",
    "- Demonstrate idempotency by running the manifest write twice without creating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ef8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "\n",
    "RAW_SAMPLE_DIR = Path(\"../queries/scheduled\")  # reuse existing SQL directory for mock payloads\n",
    "SIMULATED_DATASET = \"vehicle-positions\"\n",
    "\n",
    "def generate_mock_payload(seed: int) -> bytes:\n",
    "    random.seed(seed)\n",
    "    payload = json.dumps({\n",
    "        \"vehicle_id\": f\"veh-{seed:05d}\",\n",
    "        \"trip_id\": f\"trip-{seed:05d}\",\n",
    "        \"timestamp\": dt.datetime.utcnow().isoformat(),\n",
    "        \"lat\": -36.8485 + random.random() * 0.01,\n",
    "        \"lng\": 174.7633 + random.random() * 0.01,\n",
    "    })\n",
    "    return f\"{payload}\\n\".encode(\"utf-8\")\n",
    "\n",
    "def write_sample_objects(count: int = 5) -> list[dict]:\n",
    "    bucket = gcs_client.bucket(RAW_BUCKET)\n",
    "    written = []\n",
    "    for idx in range(count):\n",
    "        payload = generate_mock_payload(idx)\n",
    "        capture_ts = dt.datetime.utcnow()\n",
    "        partition_key = capture_ts.strftime(\"%Y-%m-%dT%H:%M\")\n",
    "        object_name = f\"{RAW_PREFIX}{capture_ts:%Y/%m/%d/%H/}{capture_ts:%M%S}-{idx:02d}.json\"\n",
    "        blob = bucket.blob(object_name)\n",
    "        blob.metadata = {\"dataset\": SIMULATED_DATASET}\n",
    "        blob.upload_from_string(payload, content_type=\"application/json\")\n",
    "        written.append({\n",
    "            \"dataset\": SIMULATED_DATASET,\n",
    "            \"object_uri\": f\"gs://{RAW_BUCKET}/{object_name}\",\n",
    "            \"capture_ts\": capture_ts.isoformat(),\n",
    "            \"partition_key\": partition_key,\n",
    "            \"status\": \"new\",\n",
    "            \"status_updated_ts\": capture_ts.isoformat(),\n",
    "            \"checksum\": blob.crc32c,\n",
    "            \"payload_bytes\": len(payload),\n",
    "            \"attributes\": {\"content_type\": blob.content_type},\n",
    "        })\n",
    "        time.sleep(0.2)  # ensure unique timestamps\n",
    "    return written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8271cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded objects:\n",
      " • gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4000-00.json 150 bytes\n",
      " • gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4001-01.json 150 bytes\n",
      " • gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4001-02.json 150 bytes\n",
      " • gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4002-03.json 151 bytes\n",
      " • gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4002-04.json 149 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761075614.728443 6836922 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_uri",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "payload_bytes",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "status_updated_ts",
         "rawType": "datetime64[us, UTC]",
         "type": "unknown"
        }
       ],
       "ref": "24e2126b-707a-4de3-81c0-03241518de6e",
       "rows": [
        [
         "0",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4002-04.json",
         "new",
         "149",
         "2025-10-21 19:40:02.528893+00:00"
        ],
        [
         "1",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4002-03.json",
         "new",
         "151",
         "2025-10-21 19:40:02.231125+00:00"
        ],
        [
         "2",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4001-02.json",
         "new",
         "150",
         "2025-10-21 19:40:01.912734+00:00"
        ],
        [
         "3",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4001-01.json",
         "new",
         "150",
         "2025-10-21 19:40:01.602847+00:00"
        ],
        [
         "4",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/4000-00.json",
         "new",
         "150",
         "2025-10-21 19:40:00.443062+00:00"
        ],
        [
         "5",
         "vehicle-positions",
         "gs://sample-bucket/raw/vehicle_positions/2025-10-21T22:00:05Z.json",
         "new",
         "12345",
         "2025-10-21 19:39:48.560749+00:00"
        ],
        [
         "6",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1237-04.json",
         "new",
         "148",
         "2025-10-21 19:12:37.183684+00:00"
        ],
        [
         "7",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-03.json",
         "new",
         "150",
         "2025-10-21 19:12:36.888786+00:00"
        ],
        [
         "8",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-02.json",
         "new",
         "149",
         "2025-10-21 19:12:36.498692+00:00"
        ],
        [
         "9",
         "vehicle-positions",
         "gs://auckland-data-dev/raw/vehicle_positions/2025/10/21/19/1236-01.json",
         "new",
         "149",
         "2025-10-21 19:12:36.191643+00:00"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>object_uri</th>\n",
       "      <th>status</th>\n",
       "      <th>payload_bytes</th>\n",
       "      <th>status_updated_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>149</td>\n",
       "      <td>2025-10-21 19:40:02.528893+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>151</td>\n",
       "      <td>2025-10-21 19:40:02.231125+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>150</td>\n",
       "      <td>2025-10-21 19:40:01.912734+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>150</td>\n",
       "      <td>2025-10-21 19:40:01.602847+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>150</td>\n",
       "      <td>2025-10-21 19:40:00.443062+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://sample-bucket/raw/vehicle_positions/2025-...</td>\n",
       "      <td>new</td>\n",
       "      <td>12345</td>\n",
       "      <td>2025-10-21 19:39:48.560749+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>148</td>\n",
       "      <td>2025-10-21 19:12:37.183684+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>150</td>\n",
       "      <td>2025-10-21 19:12:36.888786+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>149</td>\n",
       "      <td>2025-10-21 19:12:36.498692+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vehicle-positions</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/2...</td>\n",
       "      <td>new</td>\n",
       "      <td>149</td>\n",
       "      <td>2025-10-21 19:12:36.191643+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset                                         object_uri  \\\n",
       "0  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "1  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "2  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "3  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "4  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "5  vehicle-positions  gs://sample-bucket/raw/vehicle_positions/2025-...   \n",
       "6  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "7  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "8  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "9  vehicle-positions  gs://auckland-data-dev/raw/vehicle_positions/2...   \n",
       "\n",
       "  status  payload_bytes                status_updated_ts  \n",
       "0    new            149 2025-10-21 19:40:02.528893+00:00  \n",
       "1    new            151 2025-10-21 19:40:02.231125+00:00  \n",
       "2    new            150 2025-10-21 19:40:01.912734+00:00  \n",
       "3    new            150 2025-10-21 19:40:01.602847+00:00  \n",
       "4    new            150 2025-10-21 19:40:00.443062+00:00  \n",
       "5    new          12345 2025-10-21 19:39:48.560749+00:00  \n",
       "6    new            148 2025-10-21 19:12:37.183684+00:00  \n",
       "7    new            150 2025-10-21 19:12:36.888786+00:00  \n",
       "8    new            149 2025-10-21 19:12:36.498692+00:00  \n",
       "9    new            149 2025-10-21 19:12:36.191643+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_manifest_rows = write_sample_objects(count=5)\n",
    "print(\"Uploaded objects:\")\n",
    "for row in sample_manifest_rows:\n",
    "    print(\" •\", row[\"object_uri\"], row[\"payload_bytes\"], \"bytes\")\n",
    "\n",
    "upsert_manifest_rows(sample_manifest_rows)\n",
    "result_df = bq_client.query(\n",
    "    \"\"\"\n",
    "    SELECT dataset, object_uri, status, payload_bytes, status_updated_ts\n",
    "    FROM `{table}`\n",
    "    WHERE dataset = @dataset\n",
    "    ORDER BY status_updated_ts DESC\n",
    "    LIMIT 10\n",
    "    \"\"\".format(table=MANIFEST_TABLE),\n",
    "    job_config=bigquery.QueryJobConfig(query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"dataset\", \"STRING\", SIMULATED_DATASET),\n",
    "    ]),\n",
    ").to_dataframe()\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ff01a",
   "metadata": {},
   "source": [
    "## Stage 3 · Batch assembly strategies\n",
    "Stage 3 explores how to bundle manifest-listed raw objects into larger batches for downstream loading. We will benchmark two approaches:\n",
    "- **Parallel downloads** – fetch objects concurrently to a local working directory for client-side concatenation.\n",
    "- **Server-side compose** – let GCS assemble a composite object, reducing egress but subject to the 32-component limit.\n",
    "The helpers below surface the candidate manifest rows, implement both strategies, and capture timing/size metrics so we can compare trade-offs before choosing a production implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97e05be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761075618.110292 6836922 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with 8 manifest rows (status='new').\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_bytes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "duration_sec",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "destination_uri",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "452a7e4b-2d58-4f63-abee-e9ff9565b916",
       "rows": [
        [
         "0",
         "parallel_download",
         "8",
         "1197",
         "0.392",
         null
        ],
        [
         "1",
         "server_compose",
         "8",
         "1197",
         "0.414",
         "gs://auckland-data-dev/raw/vehicle_positions/batches/2025/10/21/194020667382-batch.json"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>object_count</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>destination_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parallel_download</td>\n",
       "      <td>8</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.392</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>server_compose</td>\n",
       "      <td>8</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.414</td>\n",
       "      <td>gs://auckland-data-dev/raw/vehicle_positions/b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            strategy  object_count  total_bytes  duration_sec  \\\n",
       "0  parallel_download             8         1197         0.392   \n",
       "1     server_compose             8         1197         0.414   \n",
       "\n",
       "                                     destination_uri  \n",
       "0                                                NaN  \n",
       "1  gs://auckland-data-dev/raw/vehicle_positions/b...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Sequence\n",
    "\n",
    "BATCH_PREFIX = os.environ.get(\"BATCH_PREFIX\", f\"{RAW_PREFIX}batches/\")\n",
    "\n",
    "def list_manifest_candidates(dataset: str, *, status: str = \"new\", limit: int = 16) -> list[dict]:\n",
    "    \"\"\"Fetch manifest rows for the given dataset, ordered by most recent capture.\"\"\"\n",
    "    limit = max(1, min(int(limit), 32))  # compose hard-limit is 32 components\n",
    "    query = f\"\"\"\n",
    "    SELECT dataset, object_uri, payload_bytes, capture_ts, partition_key\n",
    "    FROM `{MANIFEST_TABLE}`\n",
    "    WHERE dataset = @dataset\n",
    "      AND status = @status\n",
    "    ORDER BY capture_ts DESC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    job_config = bigquery.QueryJobConfig(query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"dataset\", \"STRING\", dataset),\n",
    "        bigquery.ScalarQueryParameter(\"status\", \"STRING\", status),\n",
    "    ])\n",
    "    df = bq_client.query(query, job_config=job_config).to_dataframe()\n",
    "    return df.to_dict(\"records\")\n",
    "\n",
    "def split_gs_uri(uri: str) -> tuple[str, str]:\n",
    "    if not uri.startswith(\"gs://\"):\n",
    "        raise ValueError(f\"Invalid GCS URI: {uri}\")\n",
    "    bucket, _, blob = uri[5:].partition(\"/\")\n",
    "    if not bucket or not blob:\n",
    "        raise ValueError(f\"Invalid GCS URI: {uri}\")\n",
    "    return bucket, blob\n",
    "\n",
    "def benchmark_parallel_download(rows: Sequence[dict], *, max_workers: int = 4) -> dict:\n",
    "    \"\"\"Download objects concurrently to a temp directory and report timing stats.\"\"\"\n",
    "    if not rows:\n",
    "        raise ValueError(\"No manifest rows supplied for parallel download.\")\n",
    "    bucket = gcs_client.bucket(RAW_BUCKET)\n",
    "    start = time.perf_counter()\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "    target_dir = Path(tmpdir.name)\n",
    "    downloaded: list[Path] = []\n",
    "\n",
    "    def _download(row: dict) -> Path:\n",
    "        bucket_name, blob_name = split_gs_uri(row[\"object_uri\"])\n",
    "        if bucket_name != RAW_BUCKET:\n",
    "            raise ValueError(f\"Unexpected bucket {bucket_name}, expected {RAW_BUCKET}.\")\n",
    "        dest_path = target_dir / Path(blob_name).name\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.download_to_filename(dest_path)\n",
    "        return dest_path\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(_download, row): row for row in rows}\n",
    "        for future in as_completed(futures):\n",
    "            downloaded.append(future.result())\n",
    "\n",
    "    duration = time.perf_counter() - start\n",
    "    total_bytes = sum(path.stat().st_size for path in downloaded)\n",
    "    tmpdir.cleanup()\n",
    "    return {\n",
    "        \"strategy\": \"parallel_download\",\n",
    "        \"object_count\": len(rows),\n",
    "        \"total_bytes\": total_bytes,\n",
    "        \"duration_sec\": round(duration, 3),\n",
    "    }\n",
    "\n",
    "def benchmark_server_compose(rows: Sequence[dict], *, prefix: str | None = None) -> dict:\n",
    "    \"\"\"Compose objects server-side into a single blob and report timing stats.\"\"\"\n",
    "    if not rows:\n",
    "        raise ValueError(\"No manifest rows supplied for server-side compose.\")\n",
    "    if len(rows) > 32:\n",
    "        raise ValueError(\"GCS compose supports at most 32 components in a single call.\")\n",
    "    bucket = gcs_client.bucket(RAW_BUCKET)\n",
    "    prefix = prefix or BATCH_PREFIX\n",
    "    if not prefix.endswith(\"/\"):\n",
    "        prefix = f\"{prefix}/\"\n",
    "    compose_name = f\"{prefix}{dt.datetime.utcnow():%Y/%m/%d/%H%M%S%f}-batch.json\"\n",
    "    destination_blob = bucket.blob(compose_name)\n",
    "    source_blobs = [bucket.blob(split_gs_uri(row[\"object_uri\"])[1]) for row in rows]\n",
    "    start = time.perf_counter()\n",
    "    destination_blob.compose(source_blobs)\n",
    "    duration = time.perf_counter() - start\n",
    "    destination_blob.reload()\n",
    "    total_bytes = destination_blob.size or sum((row.get(\"payload_bytes\") or 0) for row in rows)\n",
    "    destination_blob.delete()\n",
    "    return {\n",
    "        \"strategy\": \"server_compose\",\n",
    "        \"object_count\": len(rows),\n",
    "        \"total_bytes\": total_bytes,\n",
    "        \"duration_sec\": round(duration, 3),\n",
    "        \"destination_uri\": f\"gs://{RAW_BUCKET}/{compose_name}\",\n",
    "    }\n",
    "\n",
    "candidate_rows = list_manifest_candidates(SIMULATED_DATASET, limit=8)\n",
    "if not candidate_rows:\n",
    "    candidate_rows = write_sample_objects(count=8)\n",
    "    upsert_manifest_rows(candidate_rows)\n",
    "    candidate_rows = list_manifest_candidates(SIMULATED_DATASET, limit=8)\n",
    "\n",
    "print(f\"Benchmarking with {len(candidate_rows)} manifest rows (status='new').\")\n",
    "benchmarks = [\n",
    "    benchmark_parallel_download(candidate_rows, max_workers=4),\n",
    "    benchmark_server_compose(candidate_rows),\n",
    "]\n",
    "display(pd.DataFrame(benchmarks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9edf55",
   "metadata": {},
   "source": [
    "## Stage 4 · Batch load and manifest updates\n",
    "Stage 4 wires everything together: compose a batch artifact, load it into a BigQuery staging table, run an idempotent MERGE into the curated table, and finally flip the corresponding manifest rows to `processed`. This mirrors the production flow we expect once the manifest-driven orchestrator takes over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURATED_DATASET = os.environ.get(\"CURATED_DATASET\", BQ_DATASET)\n",
    "CURATED_TABLE = os.environ.get(\"CURATED_TABLE\", f\"{CURATED_DATASET}.vehicle_positions_curated\")\n",
    "CURATED_TABLE_FQN = f\"{PROJECT_ID}.{CURATED_TABLE}\"\n",
    "STAGING_TABLE_FQN = f\"{CURATED_TABLE_FQN}_staging\"\n",
    "\n",
    "CURATED_SCHEMA = [\n",
    "    bigquery.SchemaField(\"vehicle_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"trip_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"event_ts\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"lat\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"lng\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "MERGE_PRIMARY_FIELDS = [\"vehicle_id\", \"event_ts\"]\n",
    "\n",
    "\n",
    "def ensure_curated_tables() -> None:\n",
    "    \"\"\"Create curated and staging tables used for batch loads.\"\"\"\n",
    "    dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{CURATED_DATASET}\")\n",
    "    dataset_ref.location = \"australia-southeast1\"\n",
    "    bq_client.create_dataset(dataset_ref, exists_ok=True)\n",
    "\n",
    "    curated_table = bigquery.Table(CURATED_TABLE_FQN, schema=CURATED_SCHEMA)\n",
    "    curated_table.time_partitioning = bigquery.TimePartitioning(field=\"event_ts\", type_=bigquery.TimePartitioningType.HOUR)\n",
    "    curated_table.clustering_fields = [\"vehicle_id\"]\n",
    "    bq_client.create_table(curated_table, exists_ok=True)\n",
    "\n",
    "    staging_table = bigquery.Table(STAGING_TABLE_FQN, schema=CURATED_SCHEMA)\n",
    "    bq_client.create_table(staging_table, exists_ok=True)\n",
    "\n",
    "\n",
    "def compose_batch(rows: Sequence[dict], *, prefix: str | None = None) -> storage.Blob:\n",
    "    \"\"\"Compose manifest-listed objects into a single newline-delimited JSON blob.\"\"\"\n",
    "    if not rows:\n",
    "        raise ValueError(\"No manifest rows supplied for batch compose.\")\n",
    "    if len(rows) > 32:\n",
    "        raise ValueError(\"GCS compose supports at most 32 components per request.\")\n",
    "    bucket = gcs_client.bucket(RAW_BUCKET)\n",
    "    prefix = prefix or BATCH_PREFIX\n",
    "    if not prefix.endswith(\"/\"):\n",
    "        prefix = f\"{prefix}/\"\n",
    "    compose_name = f\"{prefix}{dt.datetime.utcnow():%Y/%m/%d/%H%M%S%f}-vehicle-positions.json\"\n",
    "    destination_blob = bucket.blob(compose_name)\n",
    "    source_blobs = [bucket.blob(split_gs_uri(row[\"object_uri\"])[1]) for row in rows]\n",
    "    destination_blob.compose(source_blobs)\n",
    "    destination_blob.reload()\n",
    "    return destination_blob\n",
    "\n",
    "\n",
    "def load_batch_to_staging(payload_rows: Sequence[dict]) -> int:\n",
    "    \"\"\"Load transformed rows into the staging table and return inserted rows.\"\"\"\n",
    "    if not payload_rows:\n",
    "        return 0\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        schema=CURATED_SCHEMA,\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "    )\n",
    "    job = bq_client.load_table_from_json(payload_rows, STAGING_TABLE_FQN, job_config=job_config)\n",
    "    job.result()\n",
    "    staging_table = bq_client.get_table(STAGING_TABLE_FQN)\n",
    "    return staging_table.num_rows or 0\n",
    "\n",
    "\n",
    "def merge_staging_into_curated() -> None:\n",
    "    \"\"\"Run an idempotent MERGE from the staging table into the curated table.\"\"\"\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE `{CURATED_TABLE_FQN}` AS curated\n",
    "    USING `{STAGING_TABLE_FQN}` AS staging\n",
    "      ON {\" AND \".join([f\"curated.{field} = staging.{field}\" for field in MERGE_PRIMARY_FIELDS])}\n",
    "    WHEN MATCHED THEN\n",
    "      UPDATE SET\n",
    "        trip_id = staging.trip_id,\n",
    "        lat = staging.lat,\n",
    "        lng = staging.lng\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (`vehicle_id`, `trip_id`, `event_ts`, `lat`, `lng`)\n",
    "      VALUES (staging.vehicle_id, staging.trip_id, staging.event_ts, staging.lat, staging.lng)\n",
    "    \"\"\"\n",
    "    bq_client.query(merge_sql).result()\n",
    "\n",
    "\n",
    "def transform_batch_blob(batch_blob: storage.Blob) -> list[dict]:\n",
    "    \"\"\"Convert batch payload into the curated schema for loading.\"\"\"\n",
    "    payload = batch_blob.download_as_text()\n",
    "    decoder = json.JSONDecoder()\n",
    "    idx = 0\n",
    "    length = len(payload)\n",
    "    transformed: list[dict] = []\n",
    "    while idx < length:\n",
    "        while idx < length and payload[idx].isspace():\n",
    "            idx += 1\n",
    "        if idx >= length:\n",
    "            break\n",
    "        record, next_idx = decoder.raw_decode(payload, idx)\n",
    "        idx = next_idx\n",
    "        transformed.append({\n",
    "            \"vehicle_id\": record[\"vehicle_id\"],\n",
    "            \"trip_id\": record[\"trip_id\"],\n",
    "            \"event_ts\": record[\"timestamp\"],\n",
    "            \"lat\": record.get(\"lat\"),\n",
    "            \"lng\": record.get(\"lng\"),\n",
    "        })\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def mark_manifest_processed(rows: Sequence[dict], *, batch_uri: str) -> None:\n",
    "    \"\"\"Update manifest rows to reflect that their payloads were loaded.\"\"\"\n",
    "    updates = []\n",
    "    now_iso = dt.datetime.utcnow().isoformat()\n",
    "    for row in rows:\n",
    "        updates.append({\n",
    "            \"dataset\": row[\"dataset\"],\n",
    "            \"object_uri\": row[\"object_uri\"],\n",
    "            \"capture_ts\": row[\"capture_ts\"],\n",
    "            \"partition_key\": row[\"partition_key\"],\n",
    "            \"status\": \"processed\",\n",
    "            \"status_updated_ts\": now_iso,\n",
    "            \"checksum\": row.get(\"checksum\"),\n",
    "            \"payload_bytes\": row.get(\"payload_bytes\"),\n",
    "            \"attributes\": {\"batch_uri\": batch_uri},\n",
    "        })\n",
    "    upsert_manifest_rows(updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "383f1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761075623.155728 6836922 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 4 using 8 manifest rows.\n",
      "Composed batch URI: gs://auckland-data-dev/raw/vehicle_positions/batches/2025/10/21/194024889710-vehicle-positions.json\n",
      "Composed batch URI: gs://auckland-data-dev/raw/vehicle_positions/batches/2025/10/21/194024889710-vehicle-positions.json\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 149 (char 148)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m batch_blob = compose_batch(candidate_rows)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComposed batch URI:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_blob.bucket.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_blob.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m payload_rows = \u001b[43mtransform_batch_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_blob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m inserted = load_batch_to_staging(payload_rows)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minserted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows into staging table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTAGING_TABLE_FQN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mtransform_batch_blob\u001b[39m\u001b[34m(batch_blob)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.strip():\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     record = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     transformed.append({\n\u001b[32m     92\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m\"\u001b[39m: record[\u001b[33m\"\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrip_id\u001b[39m\u001b[33m\"\u001b[39m: record[\u001b[33m\"\u001b[39m\u001b[33mtrip_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlng\u001b[39m\u001b[33m\"\u001b[39m: record.get(\u001b[33m\"\u001b[39m\u001b[33mlng\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     97\u001b[39m     })\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transformed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:340\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 1 column 149 (char 148)"
     ]
    }
   ],
   "source": [
    "ensure_curated_tables()\n",
    "\n",
    "candidate_rows = list_manifest_candidates(SIMULATED_DATASET, status=\"new\", limit=8)\n",
    "if not candidate_rows:\n",
    "    candidate_rows = write_sample_objects(count=8)\n",
    "    upsert_manifest_rows(candidate_rows)\n",
    "    candidate_rows = list_manifest_candidates(SIMULATED_DATASET, status=\"new\", limit=8)\n",
    "\n",
    "print(f\"Stage 4 using {len(candidate_rows)} manifest rows.\")\n",
    "batch_blob = compose_batch(candidate_rows)\n",
    "print(\"Composed batch URI:\", f\"gs://{batch_blob.bucket.name}/{batch_blob.name}\")\n",
    "\n",
    "payload_rows = transform_batch_blob(batch_blob)\n",
    "inserted = load_batch_to_staging(payload_rows)\n",
    "print(f\"Loaded {inserted} rows into staging table {STAGING_TABLE_FQN}.\")\n",
    "\n",
    "merge_staging_into_curated()\n",
    "print(f\"MERGE complete for curated table {CURATED_TABLE_FQN}.\")\n",
    "\n",
    "mark_manifest_processed(candidate_rows, batch_uri=f\"gs://{batch_blob.bucket.name}/{batch_blob.name}\")\n",
    "print(\"Manifest rows marked as processed.\")\n",
    "\n",
    "curated_preview = bq_client.query(\n",
    "    f\"\"\"\n",
    "    SELECT vehicle_id, trip_id, event_ts, lat, lng\n",
    "    FROM `{CURATED_TABLE_FQN}`\n",
    "    ORDER BY event_ts DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    ").to_dataframe()\n",
    "\n",
    "display(curated_preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
