{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb849db8",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d47d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to Python path so we can import from src modules\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285dbd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage_v1 import BigQueryWriteClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d83bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = \"regal-dynamo-470908-v9\"\n",
    "os.environ[\"BQ_DATASET\"] = \"auckland_data_dev\"\n",
    "os.environ[\"BUCKET\"] = \"auckland-data-dev\"\n",
    "\n",
    "# Encode headers as base64 (matching Terraform pattern)\n",
    "import base64\n",
    "_headers_dict = {\n",
    "  'Ocp-Apim-Subscription-Key': '1159c79486524360b17501ad888ee7d6'\n",
    "}\n",
    "os.environ[\"HEADERS\"] = base64.b64encode(json.dumps(_headers_dict).encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "os.environ[\"URL\"] = 'https://api.at.govt.nz/realtime/legacy/vehiclelocations'\n",
    "os.environ[\"DATASET\"] = 'vehicle-positions'\n",
    "os.environ[\"SPEC\"] = 'rt'\n",
    "os.environ[\"RESPONSE_TYPE\"] = 'json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf5700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"regal-dynamo-470908-v9\"\n",
    "dataset = \"auckland_data_dev\"\n",
    "table_name = 'rt_vehicle_positions'\n",
    "table_id = f\"{project_id}.{dataset}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af20e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Starting data ingestion for vehicle-positions...\n",
      "INFO - Fetched and processed data for vehicle-positions, size: 657802 bytes.\n",
      "INFO - Uploaded real-time data to GCS: rt-vehicle-positions/year=2025/month=10/day=19/hour=20/vehicle-positions-20251019T200449Z.json.gz\n",
      "INFO - Added 2 missing schema fields: ['created_at', 'updated_at'] for schema VehiclePositions\n",
      "INFO - Transformed data into DataFrame with 1861 records.\n",
      "INFO - Using deduplication mode 'skip_duplicates' for dataset 'vehicle-positions'\n",
      "INFO - Stopped prior to BigQuery upload\n"
     ]
    }
   ],
   "source": [
    "from ingest.main import run\n",
    "df = run({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0762f4c",
   "metadata": {},
   "source": [
    "## Step 1 Build Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41266f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BigQueryWriteClient initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760904294.670156 5127053 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a standard BigQuery client\n",
    "bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "# 2. Get the table object\n",
    "table = bq_client.get_table(table_id)\n",
    "schema_fields = table.schema\n",
    "\n",
    "# 3. Create the Write API client\n",
    "write_client = BigQueryWriteClient()\n",
    "print(\"\\nBigQueryWriteClient initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83c222",
   "metadata": {},
   "source": [
    "## Step 2 - Dynamically Generate a Protobuf Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ce5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated Protobuf file and message descriptors.\n"
     ]
    }
   ],
   "source": [
    "from google.protobuf import descriptor_pb2\n",
    "\n",
    "def bq_schema_to_proto_schema(\n",
    "    bq_schema: list[bigquery.SchemaField],\n",
    ") -> descriptor_pb2.FileDescriptorProto:\n",
    "    \"\"\"Converts a BigQuery schema to a Protobuf FileDescriptorProto.\"\"\"\n",
    "    \n",
    "    type_map = {\n",
    "        \"STRING\": descriptor_pb2.FieldDescriptorProto.TYPE_STRING,\n",
    "        \"BYTES\": descriptor_pb2.FieldDescriptorProto.TYPE_BYTES,\n",
    "        \"INTEGER\": descriptor_pb2.FieldDescriptorProto.TYPE_INT64,\n",
    "        \"FLOAT\": descriptor_pb2.FieldDescriptorProto.TYPE_DOUBLE,\n",
    "        \"BOOLEAN\": descriptor_pb2.FieldDescriptorProto.TYPE_BOOL,\n",
    "        \"TIMESTAMP\": descriptor_pb2.FieldDescriptorProto.TYPE_STRING, # Treat Timestamp as String\n",
    "    }\n",
    "\n",
    "    # Create a FileDescriptorProto to hold the message.\n",
    "    file_descriptor_proto = descriptor_pb2.FileDescriptorProto()\n",
    "    file_descriptor_proto.name = 'my_schema.proto'\n",
    "    file_descriptor_proto.package = 'gcp_example'\n",
    "\n",
    "    # Create the message descriptor for the row data.\n",
    "    proto_descriptor = file_descriptor_proto.message_type.add()\n",
    "    proto_descriptor.name = \"RowData\"\n",
    "\n",
    "    for i, field in enumerate(bq_schema):\n",
    "        proto_field = proto_descriptor.field.add()\n",
    "        proto_field.name = field.name\n",
    "        proto_field.number = i + 1\n",
    "        proto_field.label = descriptor_pb2.FieldDescriptorProto.LABEL_OPTIONAL\n",
    "        \n",
    "        field_type = field.field_type.upper()\n",
    "        if field_type not in type_map:\n",
    "            raise ValueError(f\"Unsupported BigQuery type: {field.field_type}\")\n",
    "            \n",
    "        proto_field.type = type_map[field_type]\n",
    "\n",
    "    # No longer need to handle timestamp dependencies\n",
    "    return file_descriptor_proto\n",
    "\n",
    "# --- Action & Verification ---\n",
    "try:\n",
    "    # This now returns a FileDescriptorProto\n",
    "    file_descriptor_proto = bq_schema_to_proto_schema(schema_fields)\n",
    "    \n",
    "    # We still need the message descriptor for the writer_schema\n",
    "    proto_schema_descriptor = file_descriptor_proto.message_type[0]\n",
    "\n",
    "    print(\"Successfully generated Protobuf file and message descriptors.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating proto schema: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e347a69",
   "metadata": {},
   "source": [
    "## Step 3 - Serialize a Single DataFrame Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c99065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import message_factory, descriptor_pool\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check empty df values\n",
    "default_values = {\n",
    "    'bearing': 0, 'odometer': 0, 'occupancy_status': 'EMPTY', 'route_id': '',\n",
    "    'trip_id': '', 'direction_id': 0, 'start_date': '', 'start_time': '',\n",
    "    'schedule_relationship': 'SCHEDULED', 'vehicle_license_plate': ''\n",
    "}\n",
    "df.fillna(value=default_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025f9637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google._upb._message.FileDescriptor at 0x128f94830>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create a FileDescriptorProto to hold our message descriptor.\n",
    "file_descriptor_proto = descriptor_pb2.FileDescriptorProto()\n",
    "file_descriptor_proto.name = 'my_schema.proto'\n",
    "file_descriptor_proto.package = 'gcp_example'\n",
    "\n",
    "# 2. Add the message descriptor (our RowData) to the file descriptor.\n",
    "file_descriptor_proto.message_type.append(proto_schema_descriptor)\n",
    "\n",
    "# 3. If the schema uses TIMESTAMP, add the required dependency.\n",
    "if any(field.field_type.upper() == \"TIMESTAMP\" for field in schema_fields):\n",
    "    if 'google/protobuf/timestamp.proto' not in file_descriptor_proto.dependency:\n",
    "        file_descriptor_proto.dependency.append('google/protobuf/timestamp.proto')\n",
    "\n",
    "# 4. Add the fully-formed file descriptor to the pool.\n",
    "pool = descriptor_pool.Default()\n",
    "pool.Add(file_descriptor_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fcb0f",
   "metadata": {},
   "source": [
    "## Step 4 - Serialize Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0461de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully serialized 1861 rows.\n",
      "Example serialized row (first 50 bytes): b'\\n\\x10308cc1f06b6854e4\\x12\\x1039b3621e78965d22\\x1a\\x192025-10-19T2'\n"
     ]
    }
   ],
   "source": [
    "# 1. Serialize all rows\n",
    "Message = message_factory.GetMessageClass(pool.FindMessageTypeByName(\"gcp_example.RowData\"))\n",
    "\n",
    "serialized_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    msg = Message()\n",
    "    for field in schema_fields:\n",
    "        name = field.name\n",
    "        if name not in row or pd.isna(row[name]):\n",
    "            continue\n",
    "\n",
    "        value = row[name]\n",
    "        \n",
    "        # All special timestamp logic is removed.\n",
    "        # The ingest function already provides datetime objects as strings.\n",
    "        if hasattr(value, 'item'):\n",
    "            value = value.item()\n",
    "        \n",
    "        # For Timestamps, ensure they are strings\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            value = value.isoformat()\n",
    "\n",
    "        setattr(msg, name, value)\n",
    "            \n",
    "    serialized_rows.append(msg.SerializeToString())\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Successfully serialized {len(serialized_rows)} rows.\")\n",
    "if serialized_rows:\n",
    "    print(f\"Example serialized row (first 50 bytes): {serialized_rows[0][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025daf3",
   "metadata": {},
   "source": [
    "## Step 5 - Create Stream and Append Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d944400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.bigquery_storage_v1 import types\n",
    "import time\n",
    "from google.protobuf import descriptor_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db52a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created write stream: projects/regal-dynamo-470908-v9/datasets/auckland_data_dev/tables/rt_vehicle_positions/streams/Cig2YmU2MWNmZS0wMDAwLTIzMTQtOTI1MC0zYzI4NmQzNjI3M2E6czEx\n",
      "Appending rows...\n",
      "Error appending rows: \n"
     ]
    }
   ],
   "source": [
    "# --- Action 1: Create a write stream ---\n",
    "parent = write_client.table_path(project_id, dataset, table_name)\n",
    "write_stream = types.WriteStream()\n",
    "write_stream.type_ = types.WriteStream.Type.COMMITTED\n",
    "\n",
    "try:\n",
    "    write_stream = write_client.create_write_stream(\n",
    "        parent=parent, write_stream=write_stream\n",
    "    )\n",
    "    stream_name = write_stream.name\n",
    "    print(f\"Successfully created write stream: {stream_name}\")\n",
    "\n",
    "    # --- Action 2: Append the serialized rows ---\n",
    "    request = types.AppendRowsRequest()\n",
    "    request.write_stream = stream_name\n",
    "    \n",
    "    # The writer_schema should contain the DescriptorProto (the message definition),\n",
    "    # not the FileDescriptorProto. The dependency is resolved by the backend\n",
    "    # because the full definition was added to the pool before serialization.\n",
    "    proto_schema = types.ProtoSchema()\n",
    "    proto_schema.proto_descriptor = proto_schema_descriptor\n",
    "\n",
    "    # Package the schema and the rows into a ProtoData object\n",
    "    proto_data = types.AppendRowsRequest.ProtoData()\n",
    "    proto_data.writer_schema = proto_schema\n",
    "    proto_data.rows.serialized_rows.extend(serialized_rows)\n",
    "    \n",
    "    # Assign the ProtoData object to the request\n",
    "    request.proto_rows = proto_data\n",
    "\n",
    "    # Send the request\n",
    "    print(\"Appending rows...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    responses = write_client.append_rows(iter([request]))\n",
    "    \n",
    "    # --- Verification ---\n",
    "    for response in responses:\n",
    "        if not response.error:\n",
    "            print(f\"Successfully appended {len(serialized_rows)} rows in {time.time() - start_time:.2f} seconds.\")\n",
    "        else:\n",
    "            print(f\"Error appending rows: {response.error.message}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
